{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3ea4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.12.129:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146e025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('deep_learning').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259c1dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the dl_data.csv into hadoop in the named folder 'user1'\n",
    "\n",
    "df = spark.read.csv('/user1/fer2013.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b31830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emotion: integer (nullable = true)\n",
      " |-- pixels: string (nullable = true)\n",
      " |-- Usage: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1c115b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+\n",
      "|emotion|              pixels|   Usage|\n",
      "+-------+--------------------+--------+\n",
      "|      0|70 80 82 72 58 58...|Training|\n",
      "|      0|151 150 147 155 1...|Training|\n",
      "|      2|231 212 156 164 1...|Training|\n",
      "|      4|24 32 36 30 32 23...|Training|\n",
      "|      6|4 0 0 0 0 0 0 0 0...|Training|\n",
      "+-------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e450134c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (35887, 3)\n"
     ]
    }
   ],
   "source": [
    "# print of data shape\n",
    "print('Shape of dataset:',(df.count(),len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "169188c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+-----------+\n",
      "|summary|           emotion|              pixels|      Usage|\n",
      "+-------+------------------+--------------------+-----------+\n",
      "|  count|             35887|               35887|      35887|\n",
      "|   mean|3.3232646919497313|                null|       null|\n",
      "| stddev|1.8738187592999593|                null|       null|\n",
      "|    min|                 0|0 0 0 0 0 0 0 0 0...|PrivateTest|\n",
      "|    max|                 6|99 99 99 99 101 1...|   Training|\n",
      "+-------+------------------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2233efde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emotion', 'pixels', 'Usage']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26c6d2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      Usage|\n",
      "+-----------+\n",
      "|   Training|\n",
      "| PublicTest|\n",
      "|PrivateTest|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 2) / 3]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "uniqe_usages = df.select(\"Usage\").distinct()\n",
    "uniqe_usages.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01775cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define mapping of emotions to labels\n",
    "emotion_mapping = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Sad\",\n",
    "    5: \"Surprise\",\n",
    "    6: \"Neutral\"\n",
    "}\n",
    "\n",
    "# Create 'label' column based on 'emotion' column\n",
    "emotion_to_label_udf = udf(lambda emotion: emotion_mapping.get(emotion, \"Unknown\"), StringType())\n",
    "\n",
    "df = df.withColumn(\"label\", emotion_to_label_udf(df[\"emotion\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39d7196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn(\"pixels\", F.split(df[\"pixels\"], \" \").cast(\"array<int>\").alias(\"pixels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e83289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emotion: integer (nullable = true)\n",
      " |-- pixels: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- Usage: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1cb5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "@udf(ArrayType(FloatType()))\n",
    "def normalize_pixels(pixels):\n",
    "    return [float(pixel) / 255.0 for pixel in pixels]\n",
    "\n",
    "df = df.withColumn(\"pixels\", normalize_pixels(df[\"pixels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f0763cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+--------+\n",
      "|emotion|              pixels|   Usage|   label|\n",
      "+-------+--------------------+--------+--------+\n",
      "|      0|[0.27450982, 0.31...|Training|   Angry|\n",
      "|      0|[0.5921569, 0.588...|Training|   Angry|\n",
      "|      2|[0.90588236, 0.83...|Training|    Fear|\n",
      "|      4|[0.09411765, 0.12...|Training|     Sad|\n",
      "|      6|[0.015686275, 0.0...|Training| Neutral|\n",
      "|      2|[0.21568628, 0.21...|Training|    Fear|\n",
      "|      4|[0.078431375, 0.0...|Training|     Sad|\n",
      "|      3|[0.3019608, 0.305...|Training|   Happy|\n",
      "|      3|[0.33333334, 0.32...|Training|   Happy|\n",
      "|      2|[1.0, 0.99607843,...|Training|    Fear|\n",
      "|      0|[0.11764706, 0.09...|Training|   Angry|\n",
      "|      6|[0.15294118, 0.29...|Training| Neutral|\n",
      "|      6|[0.85882354, 0.83...|Training| Neutral|\n",
      "|      6|[0.5803922, 0.564...|Training| Neutral|\n",
      "|      3|[0.015686275, 0.0...|Training|   Happy|\n",
      "|      5|[0.41960785, 0.41...|Training|Surprise|\n",
      "|      3|[0.05490196, 0.05...|Training|   Happy|\n",
      "|      2|[1.0, 1.0, 1.0, 1...|Training|    Fear|\n",
      "|      6|[0.5254902, 0.486...|Training| Neutral|\n",
      "|      4|[0.85882354, 0.75...|Training|     Sad|\n",
      "+-------+--------------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "781cec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "sample_pixels = df.select(\"pixels\").first()[\"pixels\"]\n",
    "print(len(sample_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e254d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+-------+\n",
      "|emotion|              pixels|   Usage|  label|\n",
      "+-------+--------------------+--------+-------+\n",
      "|      0|[0.27450982, 0.31...|Training|  Angry|\n",
      "|      0|[0.5921569, 0.588...|Training|  Angry|\n",
      "|      2|[0.90588236, 0.83...|Training|   Fear|\n",
      "|      4|[0.09411765, 0.12...|Training|    Sad|\n",
      "|      6|[0.015686275, 0.0...|Training|Neutral|\n",
      "+-------+--------------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3f3c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.filter(df[\"Usage\"] == \"Training\")\n",
    "test_df = df.filter(df[\"Usage\"] == \"PrivateTest\")\n",
    "val_df = df.filter(df[\"Usage\"] == \"PublicTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba539df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_df: (28709, 4)\n",
      "Shape of test_df: (3589, 4)\n",
      "Shape of val_df: (3589, 4)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train_df:',(train_df.count(),len(train_df.columns)))\n",
    "print('Shape of test_df:',(test_df.count(),len(test_df.columns)))\n",
    "print('Shape of val_df:',(val_df.count(),len(val_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c38f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_train = np.array([row[\"pixels\"] for row in train_df.collect()])\n",
    "y_train = np.array([row[\"emotion\"] for row in train_df.collect()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "342494b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = np.array([row[\"pixels\"] for row in test_df.collect()])\n",
    "y_test = np.array([row[\"emotion\"] for row in test_df.collect()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "315550f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "X_val = np.array([row[\"pixels\"] for row in val_df.collect()])\n",
    "y_val = np.array([row[\"emotion\"] for row in val_df.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89ba8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84324c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "898/898 [==============================] - 27s 30ms/step - loss: 1.5995 - accuracy: 0.3750 - val_loss: 1.4656 - val_accuracy: 0.4369\n",
      "Epoch 2/10\n",
      "898/898 [==============================] - 26s 29ms/step - loss: 1.3718 - accuracy: 0.4731 - val_loss: 1.3339 - val_accuracy: 0.4887\n",
      "Epoch 3/10\n",
      "898/898 [==============================] - 26s 29ms/step - loss: 1.2532 - accuracy: 0.5262 - val_loss: 1.2842 - val_accuracy: 0.5110\n",
      "Epoch 4/10\n",
      "898/898 [==============================] - 27s 30ms/step - loss: 1.1578 - accuracy: 0.5656 - val_loss: 1.2667 - val_accuracy: 0.5188\n",
      "Epoch 5/10\n",
      "898/898 [==============================] - 29s 32ms/step - loss: 1.0684 - accuracy: 0.5990 - val_loss: 1.2489 - val_accuracy: 0.5347\n",
      "Epoch 6/10\n",
      "898/898 [==============================] - 31s 34ms/step - loss: 0.9670 - accuracy: 0.6405 - val_loss: 1.2560 - val_accuracy: 0.5422\n",
      "Epoch 7/10\n",
      "898/898 [==============================] - 29s 32ms/step - loss: 0.8674 - accuracy: 0.6806 - val_loss: 1.3401 - val_accuracy: 0.5219\n",
      "Epoch 8/10\n",
      "898/898 [==============================] - 26s 29ms/step - loss: 0.7676 - accuracy: 0.7174 - val_loss: 1.3921 - val_accuracy: 0.5233\n",
      "Epoch 9/10\n",
      "898/898 [==============================] - 26s 29ms/step - loss: 0.6633 - accuracy: 0.7611 - val_loss: 1.4592 - val_accuracy: 0.5244\n",
      "Epoch 10/10\n",
      "898/898 [==============================] - 26s 29ms/step - loss: 0.5625 - accuracy: 0.7984 - val_loss: 1.6315 - val_accuracy: 0.5230\n",
      "113/113 [==============================] - 1s 11ms/step - loss: 1.6104 - accuracy: 0.5380\n",
      "Test accuracy: 0.5380328893661499\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1e16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
